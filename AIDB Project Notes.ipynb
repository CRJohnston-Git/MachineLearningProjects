{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5a1547be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100 -100 -100 -100 -100  100 -100 -100 -100 -100 -100]\n",
      "[-100   -1   -1   -1   -1   -1   -1   -1   -1   -1 -100]\n",
      "[-100   -1 -100 -100 -100 -100 -100   -1 -100   -1 -100]\n",
      "[-100   -1   -1   -1   -1   -1   -1   -1 -100   -1 -100]\n",
      "[-100 -100 -100   -1 -100 -100 -100   -1 -100 -100 -100]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "[-100 -100 -100 -100 -100   -1 -100 -100 -100 -100 -100]\n",
      "[-100   -1   -1   -1   -1   -1   -1   -1   -1   -1 -100]\n",
      "[-100 -100 -100   -1 -100 -100 -100   -1 -100 -100 -100]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "[-100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#actions:\n",
    "# 0 = up, 1 = right, 2 = down, 3 = left\n",
    "actions = [0, 1, 2, 3]\n",
    "\n",
    "row = 11 #Dynamic later, this is for testing\n",
    "col = 11 #Dynamic later, this is for testing\n",
    "wall = -100\n",
    "cheese = 100\n",
    "floor = -1\n",
    "\n",
    "#rewards and environment mapping:\n",
    "rewards = np.full((row, col), wall)\n",
    "rewards[0,5] = cheese  #cheese location for testing, change later\n",
    "envMap = {}  #mapping the maze into a dictionary, will be changed later to fit unity project\n",
    "envMap[1] = [i for i in range(1, 10)]\n",
    "envMap[2] = [1, 7, 9]\n",
    "envMap[3] = [i for i in range(1, 8)]\n",
    "envMap[3].append(9)\n",
    "envMap[4] = [3, 7]\n",
    "envMap[5] = [i for i in range (11)]\n",
    "envMap[6] = [5]\n",
    "envMap[7] = [i for i in range(1, 10)]\n",
    "envMap[8] = [3, 7]\n",
    "envMap[9] = [i for i in range(11)]\n",
    "#set rewards for aisle locations:\n",
    "for row_index in range(1, 10):\n",
    "    for column_index in envMap[row_index]:\n",
    "        rewards[row_index, column_index] = -1\n",
    "\n",
    "for row in rewards:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e2938e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define helper functions:\n",
    "#this delcares that if the rewards for the location is -1, then it is not a terminal state:\n",
    "def is_terminal_state(current_row_index, current_column_index):\n",
    "    if rewards[current_row_index, current_column_index] == -1:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "#this function chooses a random, non-terminal starting location\n",
    "def get_starting_location():\n",
    "    #get a random row and column index:\n",
    "    current_row_index = np.random.randint(11)\n",
    "    current_column_index = np.random.randint(11)\n",
    "    #choose until non-terminal state is found\n",
    "    while is_terminal_state(current_row_index, current_column_index):\n",
    "        current_row_index = np.random.randint(0, high=11)\n",
    "        current_column_index = np.random.randint(0, high=11)\n",
    "    return current_row_index, current_column_index\n",
    "#define an epsilon greedy algorithm that will choose which action to take next\n",
    "def get_next_action(current_row_index, current_column_index, epsilon):\n",
    "    #if a randomly chosen value between 0 and 1 is less than epsilon, choose the most promising value from qtable\n",
    "    if np.random.random() < epsilon:\n",
    "        q_values[0,0]\n",
    "        return np.argmax(q_values[current_row_index, current_column_index])\n",
    "    else:\n",
    "        return np.random.randint(4)\n",
    "#define a function that will get the next location beased on the chosent action\n",
    "def get_next_location(current_row_index, current_column_index, action_index):\n",
    "    new_row_index = current_row_index\n",
    "    new_column_index = current_column_index\n",
    "    if actions[action_index] == 0 and current_row_index > 0:\n",
    "        new_row_index -= 1\n",
    "    elif actions[action_index] == 1 and current_column_index < col - 1:\n",
    "        new_column_index += 1\n",
    "    elif actions[action_index] == 2 and current_row_index < row -1:\n",
    "        new_row_index += 1\n",
    "    elif actions[action_index] == 3 and current_column_index > 0:\n",
    "        new_column_index -= 1\n",
    "    return new_row_index, new_column_index\n",
    "#define a function that will get the shortest path\n",
    "def get_shortest_path(start_row_index, start_column_index):\n",
    "    #return immediately if this is an invalid starting location\n",
    "    if is_terminal_state(start_row_index, start_column_index):\n",
    "        return []\n",
    "    else:\n",
    "        current_row_index, current_column_index = start_row_index, start_column_index\n",
    "        shortest_path = []\n",
    "        shortest_path.append([current_row_index, current_column_index])\n",
    "        #continue moving along path until goal is reached\n",
    "        while not is_terminal_state(current_row_index, current_column_index):\n",
    "            action_index = get_next_action(current_row_index, current_column_index, 1)\n",
    "            current_row_index, current_column_index = get_next_location(current_row_index, current_column_index, action_index)\n",
    "            shortest_path.append([current_row_index, current_column_index])\n",
    "    return shortest_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "692f7d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "5\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m row_index, column_index \u001b[38;5;241m=\u001b[39m get_next_location(row_index, column_index, action_index)\n\u001b[0;32m     14\u001b[0m reward \u001b[38;5;241m=\u001b[39m rewards[row_index, column_index]\n\u001b[1;32m---> 15\u001b[0m old_q_value \u001b[38;5;241m=\u001b[39m q_values[old_row_index, old_column_index, action_index]\n\u001b[0;32m     16\u001b[0m temporal_difference \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m (discount_factor \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(q_values[row_index, column_index])) \u001b[38;5;241m-\u001b[39m old_q_value\n\u001b[0;32m     17\u001b[0m new_q_value \u001b[38;5;241m=\u001b[39m old_q_value \u001b[38;5;241m+\u001b[39m (learning_rate \u001b[38;5;241m*\u001b[39m temporal_difference)\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "#defining the training parameters:\n",
    "epsilon = 0.9 #this defines a 90% rate of choosing the best action, and a 10% rate of choosing a random action\n",
    "discount_factor = 0.9 #the discount rate for future rewards\n",
    "learning_rate = 0.9 #the rate that the agent learns\n",
    "#set traning epochs and train the agent:\n",
    "for epoch in range(10):\n",
    "    row_index, column_index = get_starting_location()\n",
    "    print(row_index)\n",
    "    print(column_index)\n",
    "    while not is_terminal_state(row_index, column_index):\n",
    "        action_index = get_next_action(row_index, column_index, epsilon)\n",
    "        old_row_index, old_column_index = row_index, column_index #stores old row and column information\n",
    "        row_index, column_index = get_next_location(row_index, column_index, action_index)\n",
    "        reward = rewards[row_index, column_index]\n",
    "        old_q_value = q_values[old_row_index, old_column_index, action_index]\n",
    "        temporal_difference = reward + (discount_factor * np.max(q_values[row_index, column_index])) - old_q_value\n",
    "        new_q_value = old_q_value + (learning_rate * temporal_difference)\n",
    "        q_values[old_row_index, old_column_index, action_index] = new_q_value\n",
    "print('Training complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285aae04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
