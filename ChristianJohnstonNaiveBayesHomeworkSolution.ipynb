{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e7d9d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [65.0, 80.0, 60.0, 70.0, 65.0, 60.0, 70.0, 75.0, 80.0, 75.0]\n",
      "Mean Accuracy: 70.000%\n",
      "Data=[0.035326046636964814, 0.9598933172242908, 0.4456685163423838, 0.506309313164471, 0.42666502702759324, 0.8322449044927935, 0.9769761560529061, 0.6307718366221402, 0.6950508916124665, 0.45084502714273167, 0.5238954398669781, 0.030700276754036526, 0.6749025775182691, 0.8033855134341553, 0.6598238917510797, 0.4262993787138545, 0.7374512500957098, 0.12568332230972723, 0.21213169303189394, 0.04744017352451846, 0.07072687788690923, 0.07644615553590373, 0.9171763132459553, 0.29787980574251793, 0.15820738983282634, 0.5649407226767994, 0.13039112842440792, 0.5607173210286936, 0.850526660963271, 0.5905839712874152, 0.21759033920020698, 0.9008129952123803, 0.460852490153291, 0.8279131566567798, 0.8698864279223623, 0.7800172694327171, 0.6229628138905298, 0.03742337508476401, 0.20040745546617267, 0.0990253627729546, 0.5733827030223086, 0.8965657460164287, 0.5914093121448057, 0.4923507504494802, 0.9379539017823154, 0.3900605387693097, 0.5041073672458524, 0.01720019672323503, 0.6121283611030989, 0.40232471301911965, 0.2813517154611124, 0.15696529448667917, 0.8575367430429213, 0.8111390619505175, 0.5633405356310336, 0.1351431409947249, 0.429240414597316, 0.2665354510728647, 0.09640510259345969], Predicted: 1\n"
     ]
    }
   ],
   "source": [
    "#Solution by Christian Johnston, CPSC 4383, Fall 2023\n",
    "\n",
    "#example and data provided by https://machinelearningmastery.com/standard-machine-learning-datasets/ example 4, Sonar Dataset\n",
    "\n",
    "#The purpose of this example is to use a naive bayes theorem algorithm to predict whether a series of sonar readings\n",
    "#indicate either a rock or a mine underneath the surface of the earth.\n",
    "#this dataset (as indicated by the example website) is a binary-class problem, there are 2 classes that are indicated:\n",
    "#R for rock, and M for mine.\n",
    "#The dataset has 208 observations, with 60 input variables and 1 output variable.\n",
    "#I will be using the provided k-fold cross-validation code with a k value of 10.\n",
    "\n",
    "from csv import reader\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from math import sqrt\n",
    "from math import exp\n",
    "from math import pi\n",
    "\n",
    "#Here is where I provide the data loading method.\n",
    "def loadCSV(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csvReader = reader(file)\n",
    "        for row in csvReader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    "        \n",
    "#Here are the helper functions for converting the dataset as well as the mathmatical operations for mean and stdev\n",
    "def columnStringToFloat(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())\n",
    "\n",
    "def columnStringToInt(dataset, column):\n",
    "    classValues = [row[column] for row in dataset]\n",
    "    unique = set(classValues)\n",
    "    lookup = dict()\n",
    "    for i, value in enumerate(unique):\n",
    "        lookup[value] = i\n",
    "    for row in dataset:\n",
    "        row[column] = lookup[row[column]]\n",
    "    return lookup\n",
    "\n",
    "def crossValidationSplit(dataset, nFolds):\n",
    "    datasetSplit = list()\n",
    "    datasetCopy = list(dataset)\n",
    "    foldSize  = int(len(dataset) / nFolds)\n",
    "    for _ in range(nFolds):\n",
    "        fold = list()\n",
    "        while len(fold) < foldSize:\n",
    "            index = randrange(len(datasetCopy))\n",
    "            fold.append(datasetCopy.pop(index))\n",
    "        datasetSplit.append(fold)\n",
    "    return datasetSplit\n",
    "\n",
    "def accuracyMetric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0\n",
    "\n",
    "def evaluateAlgorithm(dataset, algorithm, nFolds, *args):\n",
    "    folds = crossValidationSplit(dataset, nFolds)\n",
    "    scores = list()\n",
    "    for fold in folds:\n",
    "        trainingSet = list(folds)\n",
    "        trainingSet.remove(fold)\n",
    "        trainingSet = sum(trainingSet, [])\n",
    "        testSet = list()\n",
    "        for row in fold:\n",
    "            rowCopy = list(row)\n",
    "            testSet.append(rowCopy)\n",
    "            rowCopy[-1] = None\n",
    "        predicted = algorithm(trainingSet, testSet, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        accuracy = accuracyMetric(actual, predicted)\n",
    "        scores.append(accuracy)\n",
    "    return scores\n",
    "\n",
    "def mean(numbers):\n",
    "    return sum(numbers)/float(len(numbers))\n",
    "\n",
    "def stdev(numbers):\n",
    "    avg = mean(numbers)\n",
    "    variance = sum([(x-avg)**2 for x in numbers]) / float(len(numbers)-1)\n",
    "    return sqrt(variance)\n",
    "\n",
    "\n",
    "#following the tutorial provided by Dr. Milanova the first step is to Separate the data by class.\n",
    "def separateByClass(dataset):\n",
    "    separated = dict()\n",
    "    for i in range(len(dataset)):\n",
    "        vector = dataset[i]\n",
    "        classValue = vector[-1]\n",
    "        if (classValue not in separated):\n",
    "            separated[classValue] = list()\n",
    "        separated[classValue].append(vector)\n",
    "    return separated\n",
    "\n",
    "#the next step is to summarize the dataset by calculating the mean, stdev, and count for each column vector.\n",
    "\n",
    "def summarizeDataset(dataset):\n",
    "    summaries = [(mean(column), stdev(column), len(column)) for column in zip(*dataset)]\n",
    "    del(summaries[-1])\n",
    "    return summaries\n",
    "\n",
    "#next we split the dataset by class and calculate statistics for each row.\n",
    "def summarizeByClass(dataset):\n",
    "    separated = separateByClass(dataset)\n",
    "    summaries = dict()\n",
    "    for classValue, rows in separated.items():\n",
    "        summaries[classValue] = summarizeDataset(rows)\n",
    "    return summaries\n",
    "\n",
    "#the next step is to calculate the gaussian PDF\n",
    "def calculateProbability(x, mean, stdev):\n",
    "    exponent = exp(-((x-mean)**2 / (2 * stdev**2 )))\n",
    "    return (1 / (sqrt(2 * pi) * stdev)) * exponent\n",
    "\n",
    "#the final step uses the training data to calculate probabilities for new data\n",
    "def calculateClassProbabilities(summaries, row):\n",
    "    totalRows = sum([summaries[label][0][2] for label in summaries])\n",
    "    probabilities = dict()\n",
    "    for classValue, classSummaries in summaries.items():\n",
    "        probabilities[classValue] = summaries[classValue][0][2]/float(totalRows)\n",
    "        for i in range(len(classSummaries) - 1):\n",
    "            mean, stdev, count = classSummaries[i]\n",
    "            probabilities[classValue] *= calculateProbability(row[i], mean, stdev)\n",
    "    return probabilities\n",
    "\n",
    "#now we have the prediction function that will predict the class for the given row\n",
    "def predict(summaries, row):\n",
    "    probabilities = calculateClassProbabilities(summaries, row)\n",
    "    bestLabel, bestProb = None, -1\n",
    "    for classValue, probability in probabilities.items():\n",
    "        if bestLabel is None or probability > bestProb:\n",
    "            bestProb = probability\n",
    "            bestLabel = classValue\n",
    "    return bestLabel\n",
    "\n",
    "#here is the implementation of the naive bayes theorem\n",
    "def naiveBayes(train, test):\n",
    "    summarize = summarizeByClass(train)\n",
    "    predictions = list()\n",
    "    for row in test:\n",
    "        output = predict(summarize, row)\n",
    "        predictions.append(output)\n",
    "    return(predictions)\n",
    "\n",
    "#here I am using the provided dataset to display scores and the mean accuracy of the data\n",
    "seed(1)\n",
    "filename = 'sonarDataset.csv'\n",
    "dataset = loadCSV(filename)\n",
    "for i in range(len(dataset[0])-1):\n",
    "    columnStringToFloat(dataset, i)\n",
    "columnStringToInt(dataset, len(dataset[0])-1)\n",
    "nFolds = 10\n",
    "scores = evaluateAlgorithm(dataset, naiveBayes, nFolds)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))\n",
    "\n",
    "#now to make a prediction based on some arbitrary values for a new sonar reading (just a test)\n",
    "filename = 'sonarDataset.csv'\n",
    "dataset = loadCSV(filename)\n",
    "for i in range(len(dataset[0])-1):\n",
    "    columnStringToFloat(dataset, i)\n",
    "columnStringToInt(dataset, len(dataset[0])-1)\n",
    "model = summarizeByClass(dataset)\n",
    "# here I am adding a randomized assortment of values to create a test row\n",
    "from random import random\n",
    "row = list()\n",
    "for i in range(0, 59):\n",
    "    row.append(random())\n",
    "   \n",
    "label = predict(model, row)\n",
    "print('Data=%s, Predicted: %s' % (row, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5841119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea723513",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
